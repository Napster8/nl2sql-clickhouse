{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1cd3c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing table: customers\n",
      "Processing table: geolocation\n",
      "Processing table: geolocation\n",
      "Processing table: items\n",
      "Processing table: items\n",
      "Processing table: order_payments\n",
      "Processing table: order_payments\n",
      "Processing table: order_reviews\n",
      "Processing table: order_reviews\n",
      "Processing table: orders\n",
      "Processing table: orders\n",
      "Processing table: product_category_name_translation\n",
      "Processing table: product_category_name_translation\n",
      "Processing table: sellers\n",
      "Processing table: sellers\n",
      "Metadata saved to outputs/database_metadata.csv\n",
      "Metadata saved to outputs/database_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "import clickhouse_connect\n",
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Establish connection to clickhouse\n",
    "client = clickhouse_connect.get_client(\n",
    "    host=os.getenv(\"ch_host\"),   \n",
    "    username=os.getenv(\"ch_username\"),           \n",
    "    password=os.getenv(\"ch_password\"),\n",
    "    secure=True\n",
    ")\n",
    "\n",
    "database = os.getenv(\"ch_database\")\n",
    "\n",
    "\n",
    "def get_comprehensive_database_metadata(client, database, output_file='outputs/database_metadata.csv'):\n",
    "    \"\"\"\n",
    "    Extract comprehensive metadata for all tables and columns in the database\n",
    "    \"\"\"\n",
    "    # 1. Get all table names in the database\n",
    "    table_query = f\"SELECT name FROM system.tables WHERE database = '{database}'\"\n",
    "    table_names = [row[0] for row in client.query(table_query).result_rows]\n",
    "    \n",
    "    metadata = []\n",
    "    \n",
    "    for table in table_names:\n",
    "        print(f\"Processing table: {table}\")\n",
    "        \n",
    "        # 2. Get column information including data types and primary key status\n",
    "        column_query = f\"\"\"\n",
    "        SELECT name, type, is_in_primary_key \n",
    "        FROM system.columns \n",
    "        WHERE database = '{database}' AND table = '{table}'\n",
    "        \"\"\"\n",
    "        columns = client.query(column_query).result_rows\n",
    "        \n",
    "        for col_name, col_type, is_pk in columns:\n",
    "            try:\n",
    "                # 3. Calculate cardinality (distinct count)\n",
    "                cardinality_query = f\"SELECT count(DISTINCT `{col_name}`) FROM `{database}`.`{table}`\"\n",
    "                cardinality = client.query(cardinality_query).result_rows[0][0]\n",
    "                \n",
    "                # 4. Get distinct values based on cardinality\n",
    "                if cardinality < 30:\n",
    "                    # Get up to 3 distinct values\n",
    "                    distinct_query = f\"SELECT DISTINCT `{col_name}` FROM `{database}`.`{table}` LIMIT 3\"\n",
    "                else:\n",
    "                    # Get up to 30 distinct values\n",
    "                    distinct_query = f\"SELECT DISTINCT `{col_name}` FROM `{database}`.`{table}` LIMIT 30\"\n",
    "                \n",
    "                distinct_results = client.query(distinct_query).result_rows\n",
    "                distinct_values = [str(row[0]) for row in distinct_results]\n",
    "                \n",
    "                # Create metadata record\n",
    "                metadata.append({\n",
    "                    'table_name': table,\n",
    "                    'column_name': col_name,\n",
    "                    'data_type': col_type,\n",
    "                    'cardinality': cardinality,\n",
    "                    'primary_key': 'Yes' if is_pk else 'No',\n",
    "                    'distinct_values': ', '.join(distinct_values) if distinct_values else ''\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {table}.{col_name}: {e}\")\n",
    "                # Add record with error info\n",
    "                metadata.append({\n",
    "                    'table_name': table,\n",
    "                    'column_name': col_name,\n",
    "                    'data_type': col_type,\n",
    "                    'cardinality': 'Error',\n",
    "                    'primary_key': 'Yes' if is_pk else 'No',\n",
    "                    'distinct_values': f'Error: {str(e)}'\n",
    "                })\n",
    "    \n",
    "    # Create DataFrame and save to CSV\n",
    "    df = pd.DataFrame(metadata)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Metadata saved to {output_file}\")\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs('outputs', exist_ok=True)\n",
    "    df_metadata = get_comprehensive_database_metadata(client, database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807b15eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dspy.functional'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdspy\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdspy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InputField, OutputField, Signature\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdspy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChainOfThought\n\u001b[32m      5\u001b[39m gemini = dspy.Google(\u001b[33m\"\u001b[39m\u001b[33mmodels/gemini-1.5-flash\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# Using Gemini 1.5 Flash for efficiency\u001b[39;00m\n\u001b[32m      6\u001b[39m dspy.settings.configure(lm=gemini)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'dspy.functional'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dspy\n",
    "from dspy import InputField, OutputField, Signature\n",
    "\n",
    "# Configure Gemini as the language model in DSPy\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "gemini = dspy.Google(\"models/gemini-1.5-flash\")  # Using Gemini 1.5 Flash for efficiency\n",
    "dspy.settings.configure(lm=gemini)\n",
    "\n",
    "# DSPy Signature for Query Decomposition\n",
    "class DecomposeQuery(Signature):\n",
    "    \"\"\"Decompose a business question into atomic sub-queries for RAG retrieval.\"\"\"\n",
    "    \n",
    "    query: str = InputField(desc=\"The original user business question.\")\n",
    "    sub_queries: list[str] = OutputField(desc=\"List of 3-5 decomposed sub-queries.\")\n",
    "\n",
    "# DSPy Signature for Query Enrichment\n",
    "class EnrichSubQuery(Signature):\n",
    "    \"\"\"Enrich a sub-query with synonyms, related terms, and schema metadata.\"\"\"\n",
    "    \n",
    "    sub_query: str = InputField(desc=\"A single decomposed sub-query.\")\n",
    "    schema_metadata: str = InputField(desc=\"Optional schema metadata (e.g., table/column descriptions).\", default=\"\")\n",
    "    enriched_query: str = OutputField(desc=\"Enriched version of the sub-query with expansions.\")\n",
    "\n",
    "# Function to perform decomposition and enrichment\n",
    "def enhance_and_decompose_query(user_query, schema_metadata=\"\"):\n",
    "    # Step 1: Decompose the query using ChainOfThought for reasoning\n",
    "    decompose = dspy.ChainOfThought(DecomposeQuery)\n",
    "    decomposition_result = decompose(query=user_query)\n",
    "    sub_queries = decomposition_result.sub_queries\n",
    "    \n",
    "    # Step 2: Enrich each sub-query\n",
    "    enrich = dspy.ChainOfThought(EnrichSubQuery)\n",
    "    enriched_sub_queries = []\n",
    "    for sq in sub_queries:\n",
    "        enrichment_result = enrich(sub_query=sq, schema_metadata=schema_metadata)\n",
    "        enriched_sub_queries.append(enrichment_result.enriched_query)\n",
    "    \n",
    "    return enriched_sub_queries\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    user_query = \"What are the sales trends for top products in Q2 across regions?\"\n",
    "    schema_metadata = \"Tables include: sales_data (columns: prod_id, sales_amt, region), product_inventory (columns: prod_name, stock_level), quarterly_metrics (columns: quarter, trend_score).\"\n",
    "    \n",
    "    result = enhance_and_decompose_query(user_query, schema_metadata)\n",
    "    print(\"Enriched Sub-Queries:\")\n",
    "    for eq in result:\n",
    "        print(f\"- {eq}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SQL-on-Clickhouse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
